# -*- coding: utf-8 -*-
"""Dadess Anass.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-vSvsN-2J41xmUeUUkw_QTz94wStXb5B
"""

pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
default_of_credit_card_clients = fetch_ucirepo(id=350)

# data (as pandas dataframes)
X = default_of_credit_card_clients.data.features
y = default_of_credit_card_clients.data.targets

# metadata
print(default_of_credit_card_clients.metadata)

# variable information
print(default_of_credit_card_clients.variables)



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression

# ==============================================================================
# 1. Pr√©paration des donn√©es (Simulation bas√©e sur les r√©sultats de l'√©tude)
# ==============================================================================

# Tableau des r√©sultats r√©capitulatifs
data_resultats = {
    'M√©thode': ['R√©seau de neurones', 'Arbre de d√©cision', 'Analyse discriminante',
                'R√©gression logistique', 'K-plus proche voisin (KNN)', 'M√©thodes d‚Äôensemble'],
    'R2': [0.92, 0.85, 0.65, 0.60, 0.55, 0.88],  # R2 simul√©s (ANN le plus √©lev√©)
    'Intercept (A)': [0.01, 0.05, 0.15, 0.18, 0.20, 0.03], # A simul√©s (ANN proche de 0)
    'Slope (B)': [0.98, 0.90, 0.70, 0.65, 0.50, 0.95], # B simul√©s (ANN proche de 1)
    'Remarque': [
        'Meilleure ad√©quation, estimation fid√®le et non biais√©e',
        'Pr√©cision notable, sensible au bruit',
        'M√©thode classique, moins performante que l‚ÄôANN',
        'Robuste, mais moins pr√©cis que ANN ou arbres',
        'Pr√©cision variable selon param√©trage',
        'Souvent plus pr√©cises, complexit√© accrue'
    ]
}
df_resultats = pd.DataFrame(data_resultats)

# Simulation des donn√©es pour la visualisation de la r√©gression pour l'ANN
N = 100 # Nombre de points simul√©s
# La 'Probabilit√© r√©elle estim√©e' (Y) est notre variable d√©pendante
# La 'Probabilit√© pr√©dite' (X) est notre variable ind√©pendante
X_predit_ANN = np.linspace(0, 1, N) # Probabilit√© pr√©dite (de 0 √† 1)

# Y = A + BX + bruit. On utilise A et B du meilleur mod√®le (ANN)
A_ANN = df_resultats[df_resultats['M√©thode'] == 'R√©seau de neurones']['Intercept (A)'].iloc[0]
B_ANN = df_resultats[df_resultats['M√©thode'] == 'R√©seau de neurones']['Slope (B)'].iloc[0]
bruit = np.random.normal(0, 0.05, N) # Ajout d'un bruit normal pour simuler l'incertitude
Y_reel_estime = A_ANN + B_ANN * X_predit_ANN + bruit
# S'assurer que les probabilit√©s restent entre 0 et 1
Y_reel_estime = np.clip(Y_reel_estime, 0, 1)

# Cr√©er un DataFrame pour la r√©gression ANN
df_ann_reg = pd.DataFrame({
    'Probabilit√© Pr√©dite (X)': X_predit_ANN,
    'Probabilit√© R√©elle Estim√©e (Y)': Y_reel_estime
})

# ==============================================================================
# 2. Affichage des r√©sultats
# ==============================================================================

print("## üìä Tableau des R√©sultats des M√©thodes Compar√©es")
print(df_resultats.to_markdown(index=False))
print("\n" + "="*80 + "\n")

# ==============================================================================
# 3. Visualisation Graphique
# ==============================================================================

plt.style.use('seaborn-v0_8-whitegrid')

### 3.1. Comparaison du Coefficient de D√©termination (R¬≤)

plt.figure(figsize=(10, 6))
sns.barplot(x='R2', y='M√©thode', data=df_resultats.sort_values(by='R2', ascending=False), palette='viridis', hue='M√©thode', legend=False)
plt.title('üìà Comparaison des Coefficients de D√©termination ($R^2$)', fontsize=16)
plt.xlabel('Coefficient de D√©termination ($R^2$)', fontsize=12)
plt.ylabel('M√©thode de Data Mining', fontsize=12)

# Mettre en √©vidence la meilleure m√©thode (ANN)
for i, (r2, methode) in enumerate(zip(df_resultats.sort_values(by='R2', ascending=False)['R2'], df_resultats.sort_values(by='R2', ascending=False)['M√©thode'])):
    plt.text(r2 + 0.01, i, f'{r2:.2f}', va='center', fontsize=10, color='black',
             fontweight='bold' if methode == 'R√©seau de neurones' else 'normal')
plt.xlim(0, 1)
plt.show()


### 3.2. Droite de R√©gression Lin√©aire pour le R√©seau de Neurones (ANN)

plt.figure(figsize=(8, 8))

# Nuage de points
sns.scatterplot(x='Probabilit√© Pr√©dite (X)', y='Probabilit√© R√©elle Estim√©e (Y)', data=df_ann_reg,
                label='Donn√©es simul√©es', color='darkblue', alpha=0.6)

# Droite de r√©gression ajust√©e (Y = 0 + 1*X)
sns.lineplot(x=X_predit_ANN, y=X_predit_ANN, color='red', linestyle='--', label='Ligne Id√©ale ($Y = X$)' )

# Droite ajust√©e du mod√®le ANN simul√©
sns.regplot(x='Probabilit√© Pr√©dite (X)', y='Probabilit√© R√©elle Estim√©e (Y)', data=df_ann_reg,
            scatter=False, color='green', label=f'R√©gression ANN Simul√©e ($R^2={df_resultats.loc[0, "R2"]:.2f}$)')

plt.title('üéØ Qualit√© de la Pr√©diction par R√©seau de Neurones (ANN)', fontsize=16)
plt.xlabel('Probabilit√© Pr√©dite de D√©faut ($X$)', fontsize=12)
plt.ylabel('Probabilit√© R√©elle Estim√©e de D√©faut ($Y$)', fontsize=12)

# Afficher les coefficients de la r√©gression simul√©e
plt.text(0.05, 0.95, f'$Y = {A_ANN:.2f} + {B_ANN:.2f}X$',
         transform=plt.gca().transAxes, fontsize=12,
         bbox=dict(facecolor='white', alpha=0.7, edgecolor='green'))

plt.legend()
plt.show()


# ==============================================================================
# 4. Interpr√©tation
# ==============================================================================
print("\n## üí° Interpr√©tation des R√©sultats Graphiques")
print("* **Graphique des $R^2$ :** Il confirme que le **R√©seau de Neurones** a le $R^2$ le plus √©lev√©, indiquant que la relation lin√©aire $Y = A + BX$ entre sa pr√©diction ($X$) et la probabilit√© r√©elle estim√©e ($Y$) est la plus forte. C'est le **meilleur mod√®le** pour estimer la probabilit√© de d√©faut.")
print("* **Graphique de R√©gression (ANN) :** L'alignement des points simul√©s le long de la ligne id√©ale ($Y=X$) et les coefficients $A \approx 0$ et $B \approx 1$ montrent que le mod√®le ANN est **fid√®le et non biais√©** : une probabilit√© pr√©dite de 0.6 correspond bien √† une probabilit√© r√©elle d'environ 0.6.")

"""## Matrice de Corr√©lation des Caract√©ristiques (X)"""

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix for the features (X)
correlation_matrix = X.corr()

print("Matrice de Corr√©lation des Caract√©ristiques:")
display(correlation_matrix.head())

"""### Visualisation de la Matrice de Corr√©lation (Heatmap)"""

plt.figure(figsize=(16, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Heatmap de la Matrice de Corr√©lation des Caract√©ristiques', fontsize=18)
plt.show()

"""## Distributions des Variables Cl√©s"""

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Distribution de la variable cible 'Y' (Default Payment Next Month)
sns.countplot(x='Y', data=y, ax=axes[0])
axes[0].set_title('Distribution de la Variable Cible (Y)', fontsize=14)
axes[0].set_xlabel('D√©faut de paiement le mois prochain (0=Non, 1=Oui)', fontsize=12)
axes[0].set_ylabel('Nombre de clients', fontsize=12)

# Distribution de la variable 'X1' (Amount of the given credit)
sns.histplot(X['X1'], kde=True, ax=axes[1])
axes[1].set_title('Distribution de X1 (Montant du cr√©dit accord√©)', fontsize=14)
axes[1].set_xlabel('Montant du cr√©dit', fontsize=12)
axes[1].set_ylabel('Fr√©quence', fontsize=12)

# Distribution de la variable 'X5' (Age)
sns.histplot(X['X5'], kde=True, ax=axes[2])
axes[2].set_title('Distribution de X5 (√Çge)', fontsize=14)
axes[2].set_xlabel('√Çge', fontsize=12)
axes[2].set_ylabel('Fr√©quence', fontsize=12)

plt.tight_layout()
plt.show()